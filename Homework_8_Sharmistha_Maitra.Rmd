---
title: "COMPSCIX 415.2 Homework 8"
author: "Sharmistha Maitra"
date: "3/20/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r load_packages, warning=FALSE, message=FALSE}
library(ISLR)
library(tidyverse)
library(broom)
library(rpart)
library(partykit)
library(ROCR)
```



#__Exercise 1__
##Load the train.csv dataset into R. How many observations and columns are there? Convert the target variable to a factor because it will be loaded into R as an integer by default.
```{r}
#1460 observations and 81 columns.
Train_file_path <- "/Users/sharmisthamaitra/compscix-415-2-assignments/Train.csv"
Train_data <- read.table(Train_file_path, 
              header = TRUE,
               sep="," 
               )


Train_data_1 <- Train_data %>% mutate(Survived_fctr = case_when(
  Survived == 1 ~ 'Yes',
  Survived == 0 ~ 'No'
))
head(Train_data_1)
```

#__Exercise 2__
##Our first step is to randomly split the data into train and test datasets. We will use a 70/30 split, and use the random seed of 29283 so that we all should get the same training and test set.
```{r}
set.seed(29283)

# Let's create our training set using sample_frac. Fill in the blank.
train_set <- Train_data_1 %>% sample_frac(0.7)
head(train_set)

test_set <- Train_data_1 %>% sample_frac(0.3)
head(test_set)
```

#__Exercise 3__
##Our target is called Survived. First, fit a logistic regression model using Pclass, Sex, Fare as your three features. Fit the model using the glm() function.

##Ask yourself these questions before fitting the model:

##What kind of relationship will these features have with the probability of survival?
##Are these good features, given the problem we are trying to solve?
##After fitting the model, output the coefficients using the broom package and answer these questions:

##How would you interpret the coefficients?
##Are the features significant?
##Use the code below and fill in the blanks.
```{r}
# Fit a model with Pclass + Sex + Fare
#Even before fitting the model, it seems that Pclass , Sex and Fare will have an impact on the probability of survival. 
#For example there are more chances of better survival forpeople travelling in higher class, those who paid more fare. 

#After fitting the model, while interpreting the coefficients, we see that sex has the lowest p-value(1.490440e-35) suggesting that  sex of the passenger will have a huge impact on the probability of survival. 
#As for Fare, an increase Fare is associated with an increased probability of survival.
#A one dollar increase in the Fare increases the log-odds of survival by ~ .002.
mod_1 <- glm(Survived ~ Pclass + Sex + Fare, data = train_set, family = 'binomial')
tidy(mod_1)


```

#__Exercise 4__
##Now, let’s fit a model using a classification tree, using the same features and plot the final decision tree. Use the code below for help.
##Answer these questions:
##Describe in words one path a Titanic passenger might take down the tree. (Hint: look at your tree, choose a path from the top to a terminal node, and describe the path like this - a male passenger who paid a fare > 30 and was in first class has a high probability of survival)
##Does anything surprise you about the fitted tree?
```{r}
#A Female passenger had better chance of survival. A female passenger who was travelling in higher Pclass (>= 2.5) and paid fare between 15$ and 23.7$ has a high probability of survival. 

tree_mod <- rpart(Survived_fctr ~ Pclass + Sex + Fare, data = train_set)

plot(as.party(tree_mod))
```

#__Exercise 5__
##Evaluate both the logistic regression model and classification tree on the test_set. First, use the predict() function to get the model predictions for the testing set. Use the code below for help.


```{r}

test_logit <- predict(mod_1, newdata = test_set, type = 'response')
head(test_logit)

test_tree <- predict(tree_mod, newdata = test_set)[,2]
head(test_tree)
```

##  (a) Next, we will plot the ROC curves from both models using the code below. Don’t just copy and paste the code. Go through it line by line and see what it is doing. Recall that predictions from your decision tree are given as a two column matrix.
```{r}
# create the prediction objects for both models
pred_logit <- prediction(predictions = test_logit, labels = test_set$Survived)

pred_tree <- prediction(predictions = test_tree, labels = test_set$Survived)



# get the FPR and TPR for the logistic model
# recall that the ROC curve plots the FPR on the x-axis
perf_logit <- performance(pred_logit, measure = 'tpr', x.measure = 'fpr')
perf_logit_tbl <- tibble(perf_logit@x.values[[1]], perf_logit@y.values[[1]])

# Change the names of the columns of the tibble
names(perf_logit_tbl) <- c('fpr', 'tpr')
head(perf_logit_tbl)



# get the FPR and TPR for the tree model
perf_tree <- performance(pred_tree, measure = 'tpr', x.measure = 'fpr')
perf_tree_tbl <- tibble(perf_tree@x.values[[1]], perf_tree@y.values[[1]])

# Change the names of the columns of the tibble
names(perf_tree_tbl) <- c('fpr', 'tpr')
perf_tree_tbl



# Plotting function for plotting a nice ROC curve using ggplot
plot_roc <- function(perf_tbl) {
  p <- ggplot(data = perf_tbl, aes(x = fpr, y = tpr)) +
  geom_line(color = 'blue') +
  geom_abline(intercept = 0, slope = 1, lty = 3) +
  labs(x = 'False positive rate', y = 'True positive rate') +
  theme_bw()
  return(p)
}

# Create the ROC curves using the function we created above
plot_roc(perf_logit_tbl)
plot_roc(perf_tree_tbl)
```


##(b) Now, use the performance() function to calculate the area under the curve (AUC) for both ROC curves. Check ?performance for help on plugging in the right measure argument.
```{r}

# calculate the AUC

auc_logit <- performance(pred_logit, measure = "auc")


auc_tree <- performance(pred_tree, measure = "auc")


# extract the AUC value. AUC for pred_logit is 0.860872. #As a rule of thumb, a model with good predictive ability should have an AUC closer to 1 (1 is ideal) than to 0.5.
auc_logit@y.values[[1]]

# extract the AUC value. AUC for pred_tree is 0.8256979 . #As a rule of thumb, a model with good predictive ability should have an AUC closer to 1 (1 is ideal) than to 0.5.
auc_tree@y.values[[1]]
```


##What do you notice about the ROC curves and the AUC values? Are the models performing well? Is the logistic regression model doing better, worse, or about the same as the classification tree?

##(c) Lastly, pick a probability cutoff by looking at the ROC curves. You pick, there’s no right answer (but there is a wrong answer - make sure to pick something between 0 and 1). Using that probability cutoff, create the confusion matrix for each model by following these steps:

##Pick a cutoff value.
##Append the predicted probability values from each model (you created these at the beginning of Exercise 5) to your test_set tibble using mutate().
##Create a new column for the predicted class from each model using mutate() and case_when(). Your new predicted class columns can have two possible values: yes or no which represents whether or not the passenger is predicted to have survived or not given the predicted probability.
##You should now have 4 extra columns added to your test_set tibble, two columns of predicted probabilities, and two columns of the predicted categories based on your probability cutoff.
##Now create the table using the code below:

```{r}
test_set_1 <- test_set %>% mutate(pred_logit_prob = test_logit) %>% mutate(pred_tree_prob = test_tree)


test_set_2 <- test_set_1 %>% mutate(pred_logit_cat = case_when(pred_logit_prob < .25 ~ 'No',
                                              pred_logit_prob >= .25 ~ 'Yes'))


test_set_3 <- test_set_2  %>% mutate(pred_tree_cat = case_when(pred_tree_prob < .25 ~ 'No',
                                              pred_tree_prob >= .25 ~ 'Yes'))
head(test_set_3)

#I would say both models performing more or less the same. Logistic regression did a better job in reducing the amount of False Negatives. Decision tree did a better job is reducing the amount of false positives. 
#AUC for pred_logit is 0.860872 and AUC for pred_tree is 0.8256979 which suggests that Logistic regression is performing slightly better. 

#Logistic regression
#————------------------------
#Predicted No, Actual No - 123 (true negatives)
#Predicted No, Actual Yes - 15( false negatives)

#Predicted Yes, Actual Yes - 79 (true positives)
#Predicted Yes, Actual No - 50 ( false positives)

#Classification tree
#————----------------
#Predicted No, Actual No - 154 (true negatives)
#Predicted No, Actual Yes - 26 ( false negatives)  

#Predicted Yes, Actual Yes - 68 (true positives)
#Predicted Yes, Actual No - 19 (false positives)

test_set_3 %>% count(pred_logit_cat, Survived) %>% spread(Survived, n)
test_set_3 %>% count(pred_tree_cat, Survived) %>% spread(Survived, n)
```
